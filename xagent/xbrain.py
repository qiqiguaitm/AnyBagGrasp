import os
import base64
from openai import OpenAI
from typing import List, Dict, Union
import json
import re
import cv2
import numpy as np
from mmengine.config import Config

# Import DINO APIs
try:
    from .xdino import DetectionAPI, GraspAnythingAPI
except ImportError:
    try:
        from xdino import DetectionAPI, GraspAnythingAPI
    except ImportError:
        print("Warning: xdino module not found. Detection and grasp features will be disabled.")
        DetectionAPI = None
        GraspAnythingAPI = None


class Xbrain:
    """Robot Agent Brain for task planning and control"""
    
    def __init__(self, api_key: str = None, detection_config: Dict = None, grasp_config: Dict = None, 
                 use_default_configs: bool = True, swap_tmp_area: tuple = (0, 0)):
        """
        Initialize Xbrain with API key and DINO configs
        
        Args:
            api_key: DashScope API key. If None, will try to get from environment
            detection_config: Configuration for DetectionAPI
            grasp_config: Configuration for GraspAnythingAPI
            use_default_configs: Whether to use default configs from xdino.py
            swap_tmp_area: Temporary area coordinates (x, y) for object swapping, default (0, 0)
        """
        if api_key is None:
            api_key = os.getenv("DASHSCOPE_API_KEY")
            if not api_key:
                # Fallback to hardcoded key for testing
                api_key = "sk-e622f19390f646dab2a083ee16deb64c"
        
        if not api_key:
            raise ValueError("API key not available")
            
        self.client = OpenAI(
            api_key=api_key,
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        )
        
        # Store temporary swap area coordinates
        self.swap_tmp_area = swap_tmp_area
        
        # Initialize DINO APIs
        self.detection_api = None
        self.grasp_api = None
        
        # Use default configs from xdino.py if not provided
        if use_default_configs:
            if detection_config is None:
                detection_config = self._get_default_detection_config()
            if grasp_config is None:
                grasp_config = self._get_default_grasp_config()
        
        if DetectionAPI is not None and detection_config:
            try:
                cfg = Config()
                for key, value in detection_config.items():
                    setattr(cfg, key, value)
                self.detection_api = DetectionAPI(cfg)
                print("DetectionAPI initialized successfully")
            except Exception as e:
                print(f"Warning: Failed to initialize DetectionAPI: {e}")
        
        if GraspAnythingAPI is not None and grasp_config:
            try:
                cfg = Config()
                for key, value in grasp_config.items():
                    setattr(cfg, key, value)
                self.grasp_api = GraspAnythingAPI(cfg)
                print("GraspAnythingAPI initialized successfully")
            except Exception as e:
                print(f"Warning: Failed to initialize GraspAnythingAPI: {e}")
    
    def _get_default_detection_config(self):
        """Get default detection config from xdino.py main function"""
        return {
            'uri': '/v2/task/dinox/detection',
            'status_uri': '/v2/task_status', 
            'token': 'c4cdacb48bc4d1a1a335c88598a18e8c',
            'model_name': 'DINO-X-1.0'
        }
    
    def _get_default_grasp_config(self):
        """Get default grasp config from xdino.py main function"""
        return {
            'server_list': '/Users/didi/Documents/AgileX-DINO/AnyBagGrasp/xagent/server_grasp.json',
            'model_name': 'full'
        }
        
    def _encode_image_to_base64(self, image_path: str) -> str:
        """Encode local image to base64 data URL"""
        try:
            with open(image_path, 'rb') as image_file:
                encoded = base64.b64encode(image_file.read()).decode('utf-8')
                return f"data:image/jpeg;base64,{encoded}"
        except FileNotFoundError:
            raise FileNotFoundError(f"Image file not found: {image_path}")
    
    def _parse_action_list(self, response_text: str) -> List[Dict]:
        """
        Simple parser for VLM action list
        """
        import re
        actions = []
        
        # Find action sequence section
        match = re.search(r'\*\*åŠ¨ä½œåºåˆ—ï¼š\*\*(.*?)(?:\*\*|$)', response_text, re.DOTALL)
        action_text = match.group(1) if match else response_text
        
        # Find all pick_n_place calls
        for line in action_text.split('\n'):
            if 'pick_n_place' not in line:
                continue
                
            # Extract parameters using simple regex
            params = {}
            
            # object_id
            m = re.search(r'object_id\s*=\s*["\']([^"\']+)["\']', line)
            if m:
                params['object_id'] = m.group(1)
            
            # source_position
            if 'source_position=swap_tmp_area' in line:
                params['source_position'] = 'swap_tmp_area'
            else:
                m = re.search(r'source_position\s*=\s*\(([^)]+)\)', line)
                if m:
                    try:
                        params['source_position'] = tuple(map(float, m.group(1).split(',')))
                    except:
                        pass
            
            # target_position  
            if 'target_position=swap_tmp_area' in line:
                params['target_position'] = 'swap_tmp_area'
            else:
                m = re.search(r'target_position\s*=\s*\(([^)]+)\)', line)
                if m:
                    try:
                        params['target_position'] = tuple(map(float, m.group(1).split(',')))
                    except:
                        pass
            
            # Add action if valid
            if params.get('object_id') and params.get('target_position'):
                actions.append({
                    'type': 'pick_n_place',
                    'params': params
                })
        
        # Add reset_to_home after each pick_n_place
        final_actions = []
        for action in actions:
            final_actions.append(action)
            final_actions.append({
                'type': 'reset_to_home',
                'params': {}
            })
        
        return final_actions
    
    def _parse_object_analysis(self, response_text: str) -> Dict:
        """
        Parse detailed object analysis from the structured VLM response
        
        Args:
            response_text: Raw response text containing object analysis
            
        Returns:
            Dictionary with object IDs as keys and detailed info as values
        """
        import re
        objects = {}
        
        # Look for object analysis sections - updated to handle Chinese ID format
        # Pattern to match object ID with possible markdown formatting and full Chinese names
        # Match until we hit another section marker or end
        object_pattern = r'\*\*ç‰©ä½“ID:\s*([^*\n]+?)\*\*\s*(.*?)(?=\n---\n|\*\*ç‰©ä½“ID:|\*\*ç¬¬[äºŒä¸‰å››]æ­¥|$)'
        matches = re.findall(object_pattern, response_text, re.DOTALL)
        
        for object_id, content in matches:
            # Clean up object_id (remove trailing spaces and special chars but keep full name)
            object_id = object_id.strip().rstrip('*').strip()
            # Remove any extra colons or asterisks
            object_id = object_id.rstrip(':').strip()
            obj_info = {}
            
            # Parse each attribute - handle both [] and non-bracketed formats
            attributes = {
                'ç±»åˆ«': [r'-\s*ç±»åˆ«:\s*(.+?)(?=\n|$)', r'ç±»åˆ«:\s*(.+?)(?=\n|$)'],
                'é¢œè‰²': [r'-\s*é¢œè‰²:\s*(.+?)(?=\n|$)', r'é¢œè‰²:\s*(.+?)(?=\n|$)'],
                'å¤§å°': [r'-\s*å¤§å°:\s*(.+?)(?=\n|$)', r'å¤§å°:\s*(.+?)(?=\n|$)'],
                'ä½ç½®': [r'-\s*ä½ç½®:\s*(.+?)(?=\n|$)', r'ä½ç½®:\s*(.+?)(?=\n|$)'],

            }
            
            for attr_name, patterns in attributes.items():
                for pattern in patterns:
                    match = re.search(pattern, content)
                    if match:
                        value = match.group(1).strip()
                        # Clean up the value
                        value = value.rstrip('-').strip()
                        if value and value != '[' and not value.startswith('*'):
                            obj_info[attr_name] = value
                            break
            
            # Only add object if it has some attributes
            if obj_info:
                objects[object_id] = obj_info
        
        return objects
    
    def _parse_spatial_analysis(self, response_text: str) -> Dict:
        """
        Parse spatial relationship analysis from the VLM response
        
        Args:
            response_text: Raw response text containing spatial analysis
            
        Returns:
            Dictionary with spatial relationship information
        """
        import re
        spatial_info = {}
        
        # Look for spatial analysis sections - handle both [] and non-bracketed formats, including multiline content
        sections = {
            'æ•´ä½“å¸ƒå±€': [r'æ•´ä½“å¸ƒå±€:\s*\[([^\]]+)\]', r'æ•´ä½“å¸ƒå±€:\s*([^*\n]+(?:\n[^*\n-]+)*)'],
            'å·¦å³é¡ºåº': [r'å·¦å³é¡ºåº:\s*\[([^\]]+)\]', r'å·¦å³é¡ºåº:\s*((?:[^*\n]+(?:\n\s*-[^\n]+)*)+)'],
            'å‰åå…³ç³»': [r'å‰åå…³ç³»:\s*\[([^\]]+)\]', r'å‰åå…³ç³»:\s*([^*\n]+(?:\n[^*\n-]+)*)'],
            'ç›¸å¯¹è·ç¦»': [r'ç›¸å¯¹è·ç¦»:\s*\[([^\]]+)\]', r'ç›¸å¯¹è·ç¦»:\s*([^*\n]+(?:\n[^*\n-]+)*)'],
        }
        
        for section_name, patterns in sections.items():
            for pattern in patterns:
                match = re.search(pattern, response_text, re.DOTALL | re.MULTILINE)
                if match:
                    value = match.group(1).strip()
                    # Clean up the value - preserve line breaks for multi-line content
                    lines = value.split('\n')
                    cleaned_lines = []
                    for line in lines:
                        line = line.strip()
                        if line and line != '[' and not line.startswith('*'):
                            # Keep lines that start with dash (list items)
                            if line.startswith('-'):
                                cleaned_lines.append(line)
                            elif cleaned_lines and cleaned_lines[-1].startswith('-'):
                                # This might be a continuation of previous list item
                                cleaned_lines[-1] += ' ' + line
                            else:
                                cleaned_lines.append(line)
                    
                    if cleaned_lines:
                        spatial_info[section_name] = '\n'.join(cleaned_lines)
                        break
        
        return spatial_info
    
    def _format_grasp_info(self, grasp_points: List[Dict]) -> str:
        """
        Format grasp detection results for VLM prompt
        
        Args:
            grasp_points: List of grasp detection results
            
        Returns:
            Formatted string describing grasp information
        """
        if not grasp_points or 'error' in grasp_points[0]:
            return "æŠ“å–æ£€æµ‹: æœªæ£€æµ‹åˆ°å¯æŠ“å–å¯¹è±¡"
        
        grasp_info = []
        for i, grasp in enumerate(grasp_points):
            # Remove mask field and format key information
            grasp_desc = f"å¯¹è±¡{i+1}:"
            
            # Add category information if available
            category = grasp.get('category', 'æœªçŸ¥')
            if category and category != 'æœªçŸ¥':
                grasp_desc += f" ç±»åˆ«({category})"
            
            bbox = grasp.get('bbox', [])
            if bbox and len(bbox) >= 4:
                x1, y1, x2, y2 = bbox[:4]
                center_x = (x1 + x2) / 2
                center_y = (y1 + y2) / 2
                grasp_desc += f" ä½ç½®({center_x:.0f},{center_y:.0f})"
            
            affordances = grasp.get('affordances', [])
            if affordances:
                grasp_desc += f" æŠ“å–å€™é€‰ç‚¹{len(affordances)}ä¸ª"
                # Format first few affordances
                for j, aff in enumerate(affordances[:2]):
                    if len(aff) >= 5:
                        x, y, w, h, angle = aff[:5]
                        grasp_desc += f" ç‚¹{j+1}:({x:.0f},{y:.0f},è§’åº¦{angle:.1f})"
            
            touching_points = grasp.get('touching_points', [])
            if touching_points:
                grasp_desc += f" æ¥è§¦ç‚¹{len(touching_points)}ä¸ª"
            
            scores = grasp.get('scores', [])
            if scores:
                avg_score = sum(scores) / len(scores)
                grasp_desc += f" ç½®ä¿¡åº¦{avg_score:.2f}"
            
            grasp_info.append(grasp_desc)
        
        return "æŠ“å–æ£€æµ‹ç»“æœ:\n" + "\n".join(grasp_info)

    def _format_object_info(self, objects: List[Dict]) -> str:
        """
        Format object detection results for VLM prompt
        
        Args:
            objects: List of object detection results
            
        Returns:
            Formatted string describing object information
        """
        if not objects or 'error' in objects[0]:
            return "å¯¹è±¡æ£€æµ‹: æœªæ£€æµ‹åˆ°å¯¹è±¡"
        
        object_info = []
        for i, obj in enumerate(objects):
            # Remove mask field and format key information
            obj_desc = f"å¯¹è±¡{i+1}:"
            
            # Add category information if available
            category = obj.get('category', 'æœªçŸ¥')
            if category and category != 'æœªçŸ¥':
                obj_desc += f" ç±»åˆ«({category})"
            
            bbox = obj.get('bbox', [])
            if bbox and len(bbox) >= 4:
                x1, y1, x2, y2 = bbox[:4]
                center_x = (x1 + x2) / 2
                center_y = (y1 + y2) / 2
                width = x2 - x1
                height = y2 - y1
                obj_desc += f" ä½ç½®({center_x:.0f},{center_y:.0f}) å°ºå¯¸({width:.0f}x{height:.0f})"
            
            # Add detection confidence score
            score = obj.get('score', 0.0)
            if score > 0:
                obj_desc += f" ç½®ä¿¡åº¦{score:.2f}"
            
            object_info.append(obj_desc)
        
        return "å¯¹è±¡æ£€æµ‹ç»“æœ:\n" + "\n".join(object_info)

    def get_plan(self, image: Union[str, bytes], text: str, model: str = "qwen-vl-max-latest") -> List[Dict]:
        """
        Get action plan from image and text input, enhanced with grasp detection
        
        Args:
            image: Image file path (str) or image bytes
            text: Task description text
            model: VLM model to use
            
        Returns:
            List of action dictionaries with structure:
            [
                {
                    'type': 'action_type',
                    'description': 'human readable description',
                    'params': {'key': 'value'}
                }
            ]
        """
        try:
            # Handle image input
            if isinstance(image, str):
                if image.startswith('http'):
                    image_url = image
                else:
                    image_url = self._encode_image_to_base64(image)
            else:
                encoded = base64.b64encode(image).decode('utf-8')
                image_url = f"data:image/jpeg;base64,{encoded}"
            
            # Get object detection information
            objects = self.get_objects(image)
            #objects = self.get_grasp_points(image)
            #print(f"DEBUG: Detection API initialized: {self.detection_api is not None}")
            #print(f"DEBUG: Objects result: {objects[:2] if objects else 'None'}")
            
            # Enhance task description with object information
            object_info_text = self._format_object_info(objects)
            #object_info_text = self._format_object_info(objects)
            enhanced_text = f"ã€é‡è¦ã€‘ä»¥ä¸‹æ˜¯æ£€æµ‹ç³»ç»Ÿæä¾›çš„ç²¾ç¡®ç‰©ä½“ä¿¡æ¯ï¼Œè¯·ä¼˜å…ˆä½¿ç”¨è¿™äº›æ•°æ®ï¼š\n{object_info_text}\n\nä»»åŠ¡è¦æ±‚ï¼š{text}"
            
            # Prepare system prompt for structured output
            system_prompt = f"""ä½ æ˜¯æ™ºèƒ½æœºå™¨äººAgentå¤§è„‘ï¼Œå…·å¤‡è§†è§‰ç†è§£ã€ç©ºé—´æ¨ç†å’Œç²¾ç¡®æ“ä½œèƒ½åŠ›ã€‚

ã€é‡è¦æŒ‡ç¤ºã€‘
1. ç³»ç»Ÿå·²æä¾›ç²¾ç¡®çš„ç‰©ä½“æ£€æµ‹ä¿¡æ¯ï¼ŒåŒ…æ‹¬å‡†ç¡®çš„ä½ç½®åæ ‡ã€è¾¹ç•Œæ¡†å’Œç½®ä¿¡åº¦
2. åœ¨ç”ŸæˆåŠ¨ä½œåºåˆ—æ—¶ï¼Œå¿…é¡»ä¼˜å…ˆä½¿ç”¨æ£€æµ‹ç³»ç»Ÿæä¾›çš„åæ ‡ï¼Œè€Œéè§†è§‰ä¼°è®¡
3. ç¡®ä¿æ‰€æœ‰åæ ‡å€¼ç›´æ¥å¼•ç”¨æ£€æµ‹æ•°æ®ï¼Œæé«˜æ“ä½œç²¾åº¦

è¯·æŒ‰ç…§ä»¥ä¸‹ç»“æ„åŒ–æ ¼å¼é€æ­¥åˆ†æå¹¶è¾“å‡ºï¼š

**ç¬¬ä¸€æ­¥ï¼šé€ä¸ªç‰©ä½“è¯¦ç»†åˆ†æ**
è¯·ä»”ç»†åˆ†æå›¾åƒä¸­çš„æ¯ä¸ªç‰©ä½“ï¼Œé€ä¸ªè¾“å‡ºè¯¦ç»†ä¿¡æ¯ï¼š

**ç‰©ä½“åˆ†ææ ¼å¼ï¼ˆé€ä¸ªè¾“å‡ºï¼‰ï¼š**
ç‰©ä½“ID: [æ ¹æ®ç‰©ä½“ç‰¹å¾ç”Ÿæˆæœ‰æ„ä¹‰çš„ä¸­æ–‡åç§°ï¼Œæ ¼å¼ä¸ºï¼šé¢œè‰²+å¤§å°+ç±»åˆ«+åºå·ï¼Œå¦‚"ç²‰è‰²ä¸­ç­‰æ‰‹æè¢‹1"ã€"é»„è‰²å°å‹æ‰‹æè¢‹2"ã€"è“è‰²å¤§å‹ç›’å­3"]
- ç±»åˆ«: [å…·ä½“ç±»åˆ«åç§°ï¼Œå¦‚"æ‰‹æè¢‹"ã€"ç›’å­"ç­‰]
- é¢œè‰²: [ä¸»è¦é¢œè‰²å’Œè¾…åŠ©é¢œè‰²ï¼Œå¦‚"ç²‰è‰²ä¸»ä½“ï¼Œé»‘è‰²æ‰‹æŸ„"]
- å¤§å°: [ç›¸å¯¹å¤§å°æè¿°å’Œåƒç´ å°ºå¯¸ï¼Œå¦‚"ä¸­ç­‰å¤§å°ï¼Œçº¦120x80åƒç´ "]
- ä½ç½®: [å¿…é¡»ä½¿ç”¨æ£€æµ‹ç³»ç»Ÿæä¾›çš„ç²¾ç¡®åæ ‡ï¼Œå¦‚æ£€æµ‹åˆ°çš„"(245, 180)"ï¼Œå¹¶è¡¥å……åŒºåŸŸæè¿°]

**ç‰©ä½“IDå‘½åè§„åˆ™ï¼š**
- é¢œè‰²ï¼šç²‰è‰²ã€é»„è‰²ã€è“è‰²ã€çº¢è‰²ã€ç»¿è‰²ã€ç™½è‰²ã€é»‘è‰²ã€æ©™è‰²ã€ç´«è‰²ç­‰
- å¤§å°ï¼šå°å‹ã€ä¸­ç­‰ã€å¤§å‹ã€å¾®å°ã€å·¨å¤§
- ç±»åˆ«ï¼šæ‰‹æè¢‹ã€ç›’å­ã€ç“¶å­ã€æ¯å­ã€ä¹¦æœ¬ã€ç©å…·ç­‰
- åºå·ï¼šå¦‚æœåŒç±»ç‰©ä½“è¾ƒå¤šæ—¶æ·»åŠ æ•°å­—åç¼€
- ç¤ºä¾‹ï¼šç²‰è‰²ä¸­ç­‰æ‰‹æè¢‹1ã€é»„è‰²å°å‹ç›’å­2ã€è“è‰²å¤§å‹ä¹¦æœ¬3

**ç¬¬äºŒæ­¥ï¼šç©ºé—´å…³ç³»å’Œå¸ƒå±€åˆ†æ**
è¯¦ç»†æè¿°åœºæ™¯ä¸­å„ç‰©ä½“çš„ç©ºé—´å…³ç³»ï¼š

**å¸ƒå±€åˆ†æï¼š**
- æ•´ä½“å¸ƒå±€: [æè¿°ç‰©ä½“åœ¨åœºæ™¯ä¸­çš„æ€»ä½“åˆ†å¸ƒæ¨¡å¼]
- å·¦å³é¡ºåº: [ä»å·¦åˆ°å³åˆ—å‡ºç‰©ä½“æ’åˆ—ï¼Œå¦‚"å·¦ä¾§ï¼šç²‰è‰²è¢‹ï¼Œä¸­é—´ï¼šé»„è‰²è¢‹ï¼Œå³ä¾§ï¼šè“è‰²è¢‹"]
- å‰åå…³ç³»: [æè¿°ç‰©ä½“çš„å‰åå±‚æ¬¡ï¼Œå¦‚"æ‰€æœ‰ç‰©ä½“ä½äºåŒä¸€å¹³é¢ï¼Œæ— å‰åé®æŒ¡"]


**ç¬¬ä¸‰æ­¥ï¼šä»»åŠ¡ç†è§£å’Œå†²çªåˆ†æ**
åŸºäºè¯¦ç»†çš„ç‰©ä½“å’Œç©ºé—´åˆ†æï¼š

**ç›®æ ‡çŠ¶æ€åˆ†æï¼š**
- å½“å‰çŠ¶æ€: [è¯¦ç»†æè¿°å½“å‰ç‰©ä½“æ’åˆ—]
- ç›®æ ‡çŠ¶æ€: [è¯¦ç»†æè¿°æœŸæœ›çš„æœ€ç»ˆæ’åˆ—]
- å˜åŒ–éœ€æ±‚: [æ˜ç¡®å“ªäº›ç‰©ä½“éœ€è¦ç§»åŠ¨åˆ°å“ªé‡Œ]

**ç§»åŠ¨å†²çªè¯†åˆ«ï¼š**
- ç›´æ¥å†²çª: [å“ªäº›ç§»åŠ¨ä¼šç›´æ¥äº§ç”Ÿç©ºé—´å†²çª]
- è·¯å¾„å†²çª: [ç§»åŠ¨è·¯å¾„ä¸Šçš„æ½œåœ¨éšœç¢]
- ä¸´æ—¶åŒºåŸŸéœ€æ±‚: [æ˜¯å¦éœ€è¦ä½¿ç”¨ä¸´æ—¶æ”¾ç½®åŒºåŸŸï¼šswap_tmp_area=({self.swap_tmp_area[0]},{self.swap_tmp_area[1]})]

**ç¬¬å››æ­¥ï¼šç²¾ç¡®åŠ¨ä½œè§„åˆ’**
åŸºäºæ·±åº¦åˆ†æï¼Œè¾“å‡ºç»“æ„åŒ–åŠ¨ä½œåºåˆ—ï¼š

**å¯ç”¨APIï¼š**
- pick_n_place(object_id, source_position, target_position) - ä»æºä½ç½®æ‹¾å–ç‰©ä½“å¹¶æ”¾ç½®åˆ°ç›®æ ‡ä½ç½®
- reset_to_home() - è¿”å›åˆå§‹ä½ç½®ï¼ˆè‡ªåŠ¨æ’å…¥ï¼‰

**æ³¨æ„äº‹é¡¹ï¼š**
- object_idï¼šä½¿ç”¨ç¬¬ä¸€æ­¥åˆ†æä¸­çš„ç‰©ä½“ID
- source_positionï¼šç‰©ä½“å½“å‰ä½ç½®åæ ‡(x,y)ï¼Œä»ç¬¬ä¸€æ­¥åˆ†æçš„ä½ç½®ä¿¡æ¯ä¸­è·å–
- target_positionï¼šç›®æ ‡ä½ç½®ï¼Œå¯ä»¥æ˜¯ç²¾ç¡®åæ ‡(x,y)æˆ–swap_tmp_areaä¸´æ—¶åŒºåŸŸ

**æ‰§è¡Œç­–ç•¥ï¼š**
[è¯¦ç»†è¯´æ˜ç§»åŠ¨é¡ºåºçš„é€»è¾‘ä¾æ®ï¼Œå†²çªé¿å…æ–¹æ¡ˆï¼Œæ•ˆç‡ä¼˜åŒ–è€ƒè™‘]

**ã€é‡è¦ã€‘ä½ç½®åˆ†é…åŸåˆ™ï¼š**
- ğŸ”´ å¿…é¡»ä¼˜å…ˆä½¿ç”¨ç°æœ‰ç‰©ä½“çš„ä½ç½®æˆ–swap_tmp_areaè¿›è¡Œä½ç½®äº¤æ¢
- ğŸ”´ ä¸¥ç¦å¼€è¾Ÿæ–°çš„ç©ºé—´ä½ç½®ï¼ˆé™¤éç»å¯¹å¿…è¦ï¼‰
- ğŸ”´ å½“éœ€è¦é‡æ–°æ’åˆ—ç‰©ä½“æ—¶ï¼Œåº”è¯¥å°†ç‰©ä½“ç§»åŠ¨åˆ°å…¶ä»–ç‰©ä½“å½“å‰æ‰€åœ¨çš„ä½ç½®æˆ–swap_tmp_area
- ğŸ”´ å¦‚æœç‰©ä½“Aéœ€è¦ç§»åˆ°ä½ç½®1ï¼Œè€Œä½ç½®1è¢«ç‰©ä½“Bå ç”¨ï¼Œåˆ™å…ˆå°†Bç§»åˆ°Açš„ä½ç½®æˆ–ä¸´æ—¶åŒº
- ğŸ”´ ç›®æ ‡ä½ç½®åº”ä»å·²æœ‰çš„source_positionæˆ–swap_tmp_areaä¸­ä¸”ç©ºçš„ä½ç½®[ç‰¹åˆ«æ³¨æ„]é€‰æ‹©ï¼Œå®ç°ä½ç½®äº’æ¢

**åŠ¨ä½œåºåˆ—ï¼š**
ç¤ºä¾‹æ ¼å¼ï¼š
1. pick_n_place(object_id="ç‰©ä½“A", source_position=(738,633), target_position=swap_tmp_area) - å°†Aç§»è‡³ä¸´æ—¶åŒºï¼Œè…¾å‡ºå…¶ä½ç½®
2. pick_n_place(object_id="ç‰©ä½“B", source_position=(930,634), target_position=(738,633)) - Bç§»åˆ°AåŸæ¥çš„ä½ç½®
3. pick_n_place(object_id="ç‰©ä½“C", source_position=(1127,627), target_position=(930,634)) - Cç§»åˆ°BåŸæ¥çš„ä½ç½®
4. pick_n_place(object_id="ç‰©ä½“A", source_position=swap_tmp_area, target_position=(1127,627)) - Aä»ä¸´æ—¶åŒºç§»åˆ°CåŸæ¥çš„ä½ç½®

**è´¨é‡è¦æ±‚ï¼š**
- åæ ‡å¿…é¡»ç²¾ç¡®åˆ°åƒç´ çº§åˆ«ï¼Œé¿å…ç©ºé—´å†²çª
- ç‰©ä½“æ ‡è¯†è¦ä¸å‰é¢åˆ†æçš„ç‰©ä½“IDä¸€è‡´
- ğŸ”´ target_positionå¿…é¡»æ˜¯å·²æœ‰çš„source_positionæˆ–swap_tmp_areaä¸”ç©ºçš„ä½ç½®[ç‰¹åˆ«æ³¨æ„]
- ğŸ”´ ä¸¥æ ¼éµå¾ªä½ç½®é‡ç”¨åŸåˆ™ï¼Œä¸åˆ›å»ºæ–°ä½ç½®
- è€ƒè™‘æ“ä½œçš„ç‰©ç†å¯è¡Œæ€§å’Œå®‰å…¨æ€§
- ä¼˜åŒ–ç§»åŠ¨æ¬¡æ•°ï¼Œæé«˜æ•´ä½“æ•ˆç‡
- æ¯ä¸ªåŠ¨ä½œéƒ½è¦æœ‰æ˜ç¡®çš„æ‰§è¡Œç†ç”±"""
            print("=== VLM Prompt ===")
            print("System Prompt:\n", system_prompt)
            print("User Prompt:\n", enhanced_text)
            completion = self.client.chat.completions.create(
                model=model,
                messages=[
                    {
                        "role": "system",
                        "content": [{"type": "text", "text": system_prompt}],
                    },
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": image_url
                                },
                            },
                            {"type": "text", "text": enhanced_text},
                        ],
                    },
                ],
            )
            
            response_text = completion.choices[0].message.content
            print("=== VLM Response ===")
            print(response_text)
            
            # 1. Parse and print object analysis independently
            print("\n=== ç‰©ä½“åˆ†æ Object Analysis ===")
            object_analysis = self._parse_object_analysis(response_text)
            if object_analysis:
                print(f"æ£€æµ‹åˆ° {len(object_analysis)} ä¸ªç‰©ä½“:")
                for obj_id, details in object_analysis.items():
                    print(f"  â–º {obj_id}:")
                    if details:
                        for attr, value in details.items():
                            print(f"      {attr}: {value}")
                    print()
            else:
                print("æœªè§£æåˆ°ç‰©ä½“ä¿¡æ¯")
            
            # 2. Parse and print spatial analysis independently  
            print("\n=== ç©ºé—´åˆ†æ Spatial Analysis ===")
            spatial_analysis = self._parse_spatial_analysis(response_text)
            if spatial_analysis:
                print(f"è§£æåˆ° {len(spatial_analysis)} ä¸ªç©ºé—´å…³ç³»:")
                for section, content in spatial_analysis.items():
                    print(f"  â–º {section}: {content}")
            else:
                print("æœªè§£æåˆ°ç©ºé—´å…³ç³»")
            
            # 3. Parse and print action list independently
            print("\n=== åŠ¨ä½œåºåˆ— Action List ===")
            action_list = self._parse_action_list(response_text)
            print(f"ç”Ÿæˆ {len(action_list)} ä¸ªåŠ¨ä½œ:")
            for i, action in enumerate(action_list, 1):
                if action['type'] == 'pick_n_place':
                    params = action['params']
                    obj_id = params.get('object_id', 'unknown')
                    source = params.get('source_position', 'unknown')
                    target = params.get('target_position', 'unknown')
                    
                    # Format positions
                    if isinstance(source, tuple):
                        source_str = f"({source[0]:.0f},{source[1]:.0f})"
                    else:
                        source_str = str(source)
                    
                    if isinstance(target, tuple):
                        target_str = f"({target[0]:.0f},{target[1]:.0f})"
                    else:
                        target_str = str(target)
                    
                    print(f"  {i}. pick_n_place: '{obj_id}' ä»{source_str} -> ç§»åŠ¨åˆ°{target_str}")
                elif action['type'] == 'reset_to_home':
                    print(f"  {i}. reset_to_home: è¿”å›åˆå§‹ä½ç½®")
                else:
                    print(f"  {i}. {action['type']}")
            
            return action_list
            
        except Exception as e:
            return [
                {
                    'type': 'error',
                    'description': f'æ‰§è¡Œå¤±è´¥: {str(e)}',
                    'params': {'error': str(e)}
                }
            ]

    
    def get_objects(self, image: Union[str, np.ndarray], prompt_text: str = "pink bag. yellow bag. blue bag", 
                   bbox_threshold: float = 0.25, iou_threshold: float = 0.8) -> List[Dict]:
        """
        Detect objects in the image using DetectionAPI
        
        Args:
            image: Image file path or numpy array
            prompt_text: Natural language description of objects to detect
            bbox_threshold: Bounding box confidence threshold
            iou_threshold: IoU threshold for NMS
            
        Returns:
            List of detected objects with bbox, category, score, etc.
        """
        if self.detection_api is None:
            return [{'error': 'DetectionAPI not initialized'}]
        
        try:
            # Handle different image input types
            if isinstance(image, str):
                if image.startswith('http'):
                    # Download image for processing
                    import requests
                    response = requests.get(image)
                    rgb = cv2.imdecode(np.frombuffer(response.content, np.uint8), cv2.IMREAD_COLOR)
                    rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)
                else:
                    # Load local image
                    rgb = cv2.imread(image)
                    rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)
            elif isinstance(image, np.ndarray):
                rgb = image
            else:
                return [{'error': f'Unsupported image type: {type(image)}'}]
            
            # Perform detection
            result = self.detection_api.detect_objects(
                rgb=rgb,
                prompt_text=prompt_text,
                bbox_threshold=bbox_threshold,
                iou_threshold=iou_threshold
            )
            
            # Extract objects from result
            objects = result.get('objects', [])
            
            # Format objects for easier use
            formatted_objects = []
            for obj in objects:
                formatted_obj = {
                    'bbox': obj.get('bbox', []),
                    'category': obj.get('category', 'unknown'),
                    'score': obj.get('score', 0.0),
                    'mask': obj.get('mask', None)
                }
                formatted_objects.append(formatted_obj)
            
            return formatted_objects
            
        except Exception as e:
            return [{'error': str(e)}]
    
    def get_grasp_points(self, image: Union[str, np.ndarray], use_touching_points: bool = True, 
                        bag_mode: bool = True) -> List[Dict]:
        """
        Get grasp points for objects in the image using GraspAnythingAPI
        
        Args:
            image: Image file path or numpy array
            use_touching_points: Whether to use touching points optimization
            bag_mode: Whether to use bag-specific processing
            
        Returns:
            List of grasp information with affordances, masks, touching points, etc.
        """
        if self.grasp_api is None:
            return [{'error': 'GraspAnythingAPI not initialized'}]
        
        try:
            # Handle different image input types
            if isinstance(image, str):
                if image.startswith('http'):
                    # Download image for processing
                    import requests
                    response = requests.get(image)
                    rgb = cv2.imdecode(np.frombuffer(response.content, np.uint8), cv2.IMREAD_COLOR)
                else:
                    # Load local image
                    rgb = cv2.imread(image)
            elif isinstance(image, np.ndarray):
                rgb = image
            else:
                return [{'error': f'Unsupported image type: {type(image)}'}]
            
            # Perform grasp detection
            result, padded_img = self.grasp_api.forward(
                rgb=rgb,
                bag=bag_mode,
                use_touching_points=use_touching_points
            )
            
            # Extract grasp information from result
            grasp_objects = result[0] if result and len(result) > 0 else []
            
            formatted_grasps = []
            for obj in grasp_objects:
                formatted_obj = {
                    'bbox': obj.get('dt_bbox', []),
                    'mask': obj.get('dt_mask', None),
                    'affordances': obj.get('affs', []),
                    'scores': obj.get('scores', []),
                    'touching_points': obj.get('touching_points', [])
                }
                formatted_grasps.append(formatted_obj)
            
            return formatted_grasps
            
        except Exception as e:
            return [{'error': str(e)}]


if __name__ == "__main__":
    xbrain = Xbrain(use_default_configs=True)
    text_prompt = "è¦æ±‚äº¤æ¢å’Œè°ƒæ•´å°å‹æ‰‹æè¢‹ä½ç½®ï¼Œå®ç°ä»å·¦åˆ°å³ä¾æ¬¡æ’åˆ—pinkï¼Œyellowï¼Œblueæ’åˆ—ï¼Œä½ æ‰“ç®—å¦‚ä½•è¿›è¡Œã€‚"
    # complex scene example
    #action_list = xbrain.get_plan("example.png", text_prompt)
    # simple scene example
    action_list = xbrain.get_plan("example1.png", text_prompt, model="qwen2.5-vl-72b-instruct")
    